{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Bowwwatsaya/NCCU-s-Guide-Chatbot-/blob/main/NCCU'S_GUIDE_CHATBOT_V_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lasted\n"
      ],
      "metadata": {
        "id": "kHs-tM-P8pyv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install llama-index"
      ],
      "metadata": {
        "id": "-PgssNz_8o5L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "QfeAWbPQ8sfU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"OPENAI_API_KEY\"] = 'Please put your OPEANAI API Key here'"
      ],
      "metadata": {
        "id": "aILe7jM0QQev"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "df = pd.read_excel(\"/content/drive/MyDrive/AI/Final NCCU Map Dataset.xlsx\")\n",
        "df"
      ],
      "metadata": {
        "id": "YvKl7XMZ8xFM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.indices.struct_store import GPTPandasIndex"
      ],
      "metadata": {
        "id": "x0Kr9CYpmz7R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "JYks-Lz8k0Mg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "index = GPTPandasIndex(df=df)"
      ],
      "metadata": {
        "id": "D8iPzGhWm5ni"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query_engine = index.as_query_engine(verbose=True)"
      ],
      "metadata": {
        "id": "HbBFR2k0PLmn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query_engine = index.as_query_engine(\n",
        "    verbose=True\n",
        ")\n",
        "response = query_engine.query(\n",
        "    \"Where is Yuan,Ming,Qing Painting History\",\n",
        ")"
      ],
      "metadata": {
        "id": "HclxeUSVe0yn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index import GPTPandasIndex\n",
        "from llama_index import LLMPredictor, PromptHelper, ServiceContext\n",
        "from llama_index import QuestionAnswerPrompt\n",
        "\n",
        "# Define the LLMPredictor\n",
        "llm_predictor = LLMPredictor(llm=\"gpt-3.5-turbo\")\n",
        "\n",
        "# Define the PromptHelper\n",
        "max_input_size = 4096\n",
        "num_output = 1096\n",
        "chunk_overlap_ratio = 0.2  # Adjust as per your needs\n",
        "prompt_helper = PromptHelper(max_input_size, num_output, chunk_overlap_ratio=chunk_overlap_ratio)\n",
        "\n",
        "# Define the ServiceContext\n",
        "service_context = ServiceContext.from_defaults(llm_predictor=llm_predictor, prompt_helper=prompt_helper)\n",
        "\n",
        "# Create the GPTPandasIndex with the ServiceContext\n",
        "index = GPTPandasIndex(df=df, service_context=service_context)\n",
        "\n",
        "# Create the QueryEngine\n",
        "query_engine = index.as_query_engine(verbose=True)"
      ],
      "metadata": {
        "id": "F3owTZXNe-5D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index import QuestionAnswerPrompt, RefinePrompt\n",
        "\n",
        "my_qa_prompt = (\n",
        "    \"Please refer to the following information:\\n\"\n",
        "    \"------------\\n\"\n",
        "    \"{context_str}\\n\"\n",
        "    \"------------\\n\"\n",
        "    \"You have to answer the questions based on the df dataframe.\\n\"\n",
        ")\n",
        "\n",
        "QA_PROMPT = QuestionAnswerPrompt(my_qa_prompt)\n",
        "\n",
        "def ask_ai():\n",
        "    while True:\n",
        "        query = input(\"Do you have any questions about NCCU buildings?\\n\")\n",
        "        query_engine = index.as_query_engine(response_mode=\"compact\", mode=\"embedding\", text_qa_template=QA_PROMPT, verbose=True, similarity_top_k=3)\n",
        "        response = query_engine.query(query)\n",
        "        print(f\"\\nAnswer: {response.response}\\n\")"
      ],
      "metadata": {
        "id": "f5tmy7AtfZkz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install gradio"
      ],
      "metadata": {
        "id": "CuIjw0KbnDO6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr"
      ],
      "metadata": {
        "id": "YIswDPDvn7K9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "\n",
        "def ask_ai(query):\n",
        "    query_engine = index.as_query_engine(response_mode=\"compact\", mode=\"embedding\", text_qa_template=QA_PROMPT, verbose=True, similarity_top_k=3)\n",
        "    response = query_engine.query(query)\n",
        "    return response.response\n",
        "\n",
        "iface = gr.Interface(fn=ask_ai, inputs=\"text\", outputs=\"text\", title=\"NCCU'S GUIDE CHAT BOT\")\n",
        "iface.launch(share=True)"
      ],
      "metadata": {
        "id": "wnxfN7xFoKTs"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}